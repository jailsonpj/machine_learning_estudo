{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Learning refere-se ao processo de construção de múltiplos modelos e , em seguida, combina-os de uma forma que produz melhores resultados do que os modelos individuais. Esses modelos individuais podem ser classificadores, regressores ou qualquer outra coisa que modifique dados de alguma forma. O aprendizado do Ensemble é usado extensivamente em vários campos, incluindo classificação de dados, modelagem preditiva, detecção de anomalias e assim por diante.\n",
    "\n",
    "Porque precisamos de ensemble aprendizado em primeiro lugar? Para entender isso, vamos dar um exemplo da vida real. Você quer comprar uma nova TV, mas não sabe quais são os modelos mais recentes. Seu objetivo é obter o melhor valor pelo seu dinheiro, mas você não tem conhecimento suficiente sobre esse assunto para tomar uma decisão informada. Quando você tem que tomar uma decisão sobre algo assim, você vai ao redor e tenta obter as opiniões de vários especialistas no domínio. Isso ajudará você a tomar a melhor decisão. Mais frequentemente do que não, em vez de apenas confiar em uma única opinião, você tende a tomar uma decisão final combinando as decisões individuais desses especialistas. A razão pela qual fazemos isso é porque queremos minimizar a possibilidade de uma decisão errada ou sub-ótima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando selecionamos um modelo, o procedimento mais comumente usado é escolher aquele com o menor erro no conjunto de dados de treinamento. O problema com essa abordagem é que nem sempre funcionará. O modelo pode ser tendencioso ou superestimar os dados de treinamento. Mesmo quando calculamos o modelo usando validação cruzada, ele pode ter um desempenho ruim em dados desconhecidos.\n",
    "\n",
    "Uma das principais razões pelas quais a aprendizagem conjunta é tão eficaz é porque ela reduz o risco geral de fazer uma seleção de modelo ruim. Isso permite que ele treine de maneira diversa e tenha um bom desempenho em dados desconhecidos. Quando construímos um modelo usando aprendizado conjunto, os modelos individuais precisam exibir alguma diversidade. Isso permitiria que eles capturassem várias nuances em nossos dados; daí o modelo geral se torna mais preciso.\n",
    "\n",
    "A diversidade é obtida usando diferentes parâmetros de treinamento para cada modelo individual. Isso permite que modelos individuais gerem diferentes limites de decisão para dados de treinamento. Isso significa que cada modelo usará regras diferentes para fazer uma inferência, que é uma maneira poderosa de validar o resultado final. Se houver concordância entre os modelos, sabemos que a saída está correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árvore de Decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma árvore de decisão é uma estrutura que nos permite dividir o conjunto de dados em ramificações e, em seguida, tomar decisões simples em cada nível. Isso nos permitirá chegar à decisão final de descer a árvore. As árvores de decisão são produzidas por algoritmos de treinamento , que identificam como podemos dividir os dados da melhor maneira possível.\n",
    "\n",
    "Qualquer processo de decisão começa no nó raiz no topo da árvore. Cada nó da árvore é basicamente uma regra de decisão. Os algoritmos constroem essas regras com base na relação entre os dados de enntrada e os rótulos de destino nos dados de treinamento. Os valores nos dados de entrada são utilizados para estimar o valor da saída.\n",
    "\n",
    "A árvore de decisão são modelos de aprendizado de máquina que estão na forma de estruturas de árvore. Cada nó não-folha nesta árvore é basicamente um tomador de decisão. Esses nós são chamados de nós de decisão. Cada nó realiza um teste específico para determinar onde ir em seguida. Dependendo do resultado, você vai para o ramo da direita ou para o ramo da esquerda este nó. Continuamos fazendo isso até chegarmos a um nó da folha. Se estamos construindo um classificador, cada nó da folha representa uma classe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como podemos construir automaticamente essa árvore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é a parte complicada! Compreender o conceito de árvores de decisão é bastante simples, mas como construímos a árvore ideal? Qual atributo deve estar no nó raiz? Como decidimos os limiares? Para entender isso, devemos primeiro discutir o conceito de entropia.\n",
    "\n",
    "A entropia é basicamente uma medida de incerteza. À medida que nos movemos do topo da árvore para o fundo, passamos da incerteza total para a certeza completa. Quando começamos no topo, não sabemos nada sobre o ponto de dados de entrada. Quando vamos para o nó folha, sabemos exatamente a que classe o ponto de dados pertence. Portanto, uma boa maneira de construir a árvore de decisão seria reduzir a incerteza em cada nível. Em outras palavras, precisamos reduzir a entropia com cada nível de passagem na árvore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como medir a entropia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reduzir a entropia, primeiro precisamos saber como medi-la. Entropia é definida como:\n",
    "    \n",
    "    entropia (X) = -∑p i log 2 p i\n",
    "    \n",
    "onde p i refere-se à probabilidade de i ocorrer no conjunto de dados e a soma é sobre todos os símbolos distintos no conjunto de dados.\n",
    "\n",
    "Para ter uma ideia sobre o seu comportamento, vamos considerar um conjunto de dados com 60 itens. Neste conjunto de dados, existem 3 tipos de itens. Agora, se cada item aparece 20 vezes neste conjunto de dados, a entropia é alta porque há muita incerteza. Como em, se você escolher aleatoriamente um item, existe uma chance igual de obter qualquer um dos três itens. Por outro lado, se o primeiro item aparecer 58 vezes e os dois itens restantes aparecerem 1 vez cada, então a incerteza é realmente baixa. Se você escolher aleatoriamente uma amostra, pode dizer com certeza que será o primeiro item. Então, para reduzir a entropia, precisamos que o conjunto de dados seja distorcido de alguma forma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como reduzir a entropia na árvore de decisão?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar o exemplo do conjunto de dados com 60 itens em que o primeiro item aparece 14 vezes, o segundo item aparece 27 vezes e o terceiro item aparece 19 vezes. Vamos medir a entropia:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0630322522072195\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "entropia = - (14/60) * math.log(14/60) -(27/60) * math.log(27/60) - (19/60) * math.log(19/60)\n",
    "print(entropia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma árvore de decisão, precisamos dividir este conjunto de dados em duas partes para reduzir a entropia geral. Aqui estão as divisões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_esquerda = [5,21,11]\n",
    "sub_direita = [9,6,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos medir a entropia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropia esquerda:  0.952566324667347\n",
      "entropia direita:  1.08501115745815\n"
     ]
    }
   ],
   "source": [
    "entropia_esq = - (5/37) * math.log(5/37) - (21/37) * math.log(21/37) - (11/37) * math.log(11/37)\n",
    "entropia_dir = - (9/23) * math.log(9/23) - (6/23) * math.log(6/23) - (8/23) * math.log(8/23)\n",
    "print(\"entropia esquerda: \",entropia_esq)\n",
    "print(\"entropia direita: \",entropia_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para medir a entropia geral das duas subárvores, precisamos tomar a soma ponderada com base no número de itens em cada subárvore. A subárvore esquerda tem 37 itens e a subárvore direita tem 23 itens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropia geral:  1.0033368439038215\n"
     ]
    }
   ],
   "source": [
    "entropia_geral = (37/60) * entropia_esq + (23/60) * entropia_dir\n",
    "print(\"entropia geral: \",entropia_geral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos calcular a diferença entre a entropia inicial e a entropia final que acabamos de calcular. Isso é chamado de \"ganho de informação\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ganho de Informação:  0.059695408303398034\n"
     ]
    }
   ],
   "source": [
    "ganho_de_informacao = entropia - entropia_geral\n",
    "print(\"Ganho de Informação: \",ganho_de_informacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, isso nos diz que, se dividirmos o conjunto de dados dessa maneira específica, obteremos um ganho de informação de 0,05. Em cada nó, fazemos isso para todos os recursos do vetor de recursos e escolhemos com avidez aquele que fornece o maior ganho de informações. É assim que decidimos qual atributo vai no topo da árvore de decisão e o que vem depois disso. Cada recurso divide o conjunto de dados de uma maneira diferente, portanto, precisamos verificar todos eles para escolher o máximo. Computar isto é computacionalmente barato, portanto as árvores de decisão são muito rápidas de treinar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
