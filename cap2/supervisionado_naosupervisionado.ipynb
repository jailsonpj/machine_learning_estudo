{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado supervisionado versus não supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizado supervisionado refere-se ao processo de construção de um modelo de aprendizado de máquina baseado em dados de treinamento rotulados. Por exemplo, digamos que queremos construir um sistema para prever automaticamente a renda de uma pessoa, com base em vários parâmetros, como idade, escolaridade, localização e assim por diante. Para fazer isso, precisamos criar um banco de dados de pessoas com todos os detalhes necessários e rotulá-lo. Ao fazer isso, estamos dizendo ao nosso algoritmo quais parâmetros correspodem a qual renda. com base nesse mapeamento, o algoritmo aprenderá a calcular a renda de uma pessoa usando os parâmetros fornecidos a ela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizado não supervisionado refere-se ao processo de construção de um modelo de aprendizado de máquina sem depender de dados de treinamento rotulados. Em certo sentido, é o oposto do que acabamos de discutir no aprendizado supervisionado. Como não há rótulos disponíveis, você precisa extrair insights com base apenas nos dados fornecidos a você. Por exemplo, digamos que queremos construir um sistema em que temos que separar um conjunto de pontos de dados em vários grupos. O mais complicado aqui é que não sabemos exatamente quais devem ser os critérios de separação. Portanto, um algoritmo de aprendizado não supervisionado precisa separar o conjunto de dados em um número de grupos da melhor maneira possivel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de classificação é uma técnica em que classificamos dados em um determinado número de classes. Durante a classificação, organizamos os dados em um número fixo de categorias, para que possam ser usados de maneira mais eficaz e eficiente.\n",
    "No aprendizado de máquina, a classificação resolve o problema de identificar a categoria à qual um novo ponto de dados pertence. Construímos o modelo de classificação com base no conjunto de dados de treinamento contendo pontos de dados e os rótulos correspondentes.Por exemplo, digamos que queremos verificar se a imagem dada contém a face de uma pessoa ou não. Construiríamos um conjunto de dados de treinamento contendo classes correspondentes a essas duas classes: facee no-face. Em seguida, treinamos o modelo com base nas amostras de treinamento que temos. Este modelo treinado é então usado para inferência.\n",
    "Precisamos fornecer um número suficientemente grande de amostras para que possa generalizar esses critérios. Se houver um número insuficiente de amostras, o algoritmo se ajustará aos dados de treinamento. Isso significa que ele não terá um bom desempenho em dados desconhecidos porque ajustou muito o modelo para se ajustar aos padrões observados nos dados de treinamento. Este é realmente um problema muito comum que ocorre no mundo do aprendizado de máquina. É bom considerar esse fator quando você cria vários modelos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os algoritmos de aprendizado de máquina esperam que os dados sejam formatados de uma determinada maneira antes de iniciar o processo de treinamento. Para preparar os dados para ingestão por algoritmos de aprendizado de máquina, temos que pré-processá-lo e convertê-lo no formato correto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([[5.1, -2.9, 3.3], \n",
    "                       [-1.2, 7.8, -6.1], \n",
    "                       [3.9, 0.4, 2.1], \n",
    "                       [7.3, -9.9, -4.5]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Binarização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse processo é usado quando queremos converter nossos valores numéricos em valores booleanos. Vamos usar um método embutido para binarizar dados de entrada usando 2.1como valor limite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binarized data:\n",
      " [[1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Binarize data \n",
    "data_binarized = preprocessing.Binarizer(threshold=2.1).transform(input_data)\n",
    "print(\"\\nBinarized data:\\n\",data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver aqui, todos os valores acima 2.1 se tornam 1. Os valores restantes se tornam 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Remoção Média"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remover a média é uma técnica de pré-processamento comum usada no aprendizado de máquina. Geralmente, é útil remover a média do nosso vetor de recursos, para que cada recurso seja centralizado em zero. Fazemos isso para remover o viés dos recursos do nosso vetor de recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before\n",
      "Mean = [ 3.775 -1.15  -1.3  ]\n",
      "Std deviation = [3.12039661 6.36651396 4.0620192 ]\n"
     ]
    }
   ],
   "source": [
    "# print mean and standard deviation \n",
    "print(\"\\nBefore\")\n",
    "print(\"Mean =\",input_data.mean(axis=0))\n",
    "print(\"Std deviation =\",input_data.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.1 -2.9  3.3]\n",
      " [-1.2  7.8 -6.1]\n",
      " [ 3.9  0.4  2.1]\n",
      " [ 7.3 -9.9 -4.5]]\n",
      "\n",
      "After:\n",
      "Mean = [1.11022302e-16 0.00000000e+00 2.77555756e-17]\n",
      "Std deviation = [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Remove mean\n",
    "print(input_data)\n",
    "data_scaled = preprocessing.scale(input_data)\n",
    "print(\"\\nAfter:\")\n",
    "print(\"Mean =\",data_scaled.mean(axis=0))\n",
    "print(\"Std deviation =\",data_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso vetor de recursos, o valor de cada recurso pode variar entre muitos valores aleatórios. Por isso, torna-se importante dimensionar esses recursos para que seja um campo de atuação nivelado para o algoritmo de aprendizado de máquina treinar. Não queremos que nenhum recurso seja artificialmente grande ou pequeno apenas devido à natureza das medições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min max scaled data:\n",
      " [[0.74117647 0.39548023 1.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.6        0.5819209  0.87234043]\n",
      " [1.         0.         0.17021277]]\n"
     ]
    }
   ],
   "source": [
    "# Min max scaling \n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0, 1)) \n",
    "data_scaled_minmax = data_scaler_minmax.fit_transform(input_data) \n",
    "print(\"\\nMin max scaled data:\\n\", data_scaled_minmax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Normalzação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos o processo de normalização para modificar os valores no vetor de recursos para que possamos medi-los em uma escala comum. No aprendizado de máquina, usamos muitas formas diferentes de normalização. Algumas das formas mais comuns de normalização visam modificar os valores de modo que eles se somam 1. A normalização de L1 , que se refere a desvios absolutos mínimos , funciona certificando-se de que a soma dos valores absolutos é 1 em cada linha. A normalização de L2 , que se refere a mínimos quadrados, funciona certificando-se de que a soma dos quadrados é 1 .\n",
    "\n",
    "Em geral, a técnica de normalização L1 é considerada mais robusta do que a técnica de normalização L2. A técnica de normalização de L1 é robusta porque é resistente a valores discrepantes nos dados. Muitas vezes, os dados tendem a conter outliers e não podemos fazer nada sobre isso. Queremos usar técnicas que possam seguramente e efetivamente ignorá-las durante os cálculos. Se estivermos resolvendo um problema em que os valores discrepantes são importantes, talvez a normalização de L2 se torne uma opção melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L1 normalized data:\n",
      " [[ 0.45132743 -0.25663717  0.2920354 ]\n",
      " [-0.0794702   0.51655629 -0.40397351]\n",
      " [ 0.609375    0.0625      0.328125  ]\n",
      " [ 0.33640553 -0.4562212  -0.20737327]]\n",
      "\n",
      "L2 normalized data:\n",
      " [[ 0.75765788 -0.43082507  0.49024922]\n",
      " [-0.12030718  0.78199664 -0.61156148]\n",
      " [ 0.87690281  0.08993875  0.47217844]\n",
      " [ 0.55734935 -0.75585734 -0.34357152]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data \n",
    "data_normalized_l1 = preprocessing.normalize(input_data, norm='l1') \n",
    "data_normalized_l2 = preprocessing.normalize(input_data, norm='l2') \n",
    "print(\"\\nL1 normalized data:\\n\", data_normalized_l1) \n",
    "print(\"\\nL2 normalized data:\\n\", data_normalized_l2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
